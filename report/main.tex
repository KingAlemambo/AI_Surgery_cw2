\documentclass[runningheads]{llncs}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{hyperref}

% For placeholder text - remove in final version
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}
\newcommand{\result}[1]{\textcolor{blue}{#1}}

\begin{document}

\title{Temporal Context for Surgical Workflow Analysis: Duration Prediction and Tool Detection in Cholecystectomy}

\author{Alexandros Mathios}
\institute{University College London, Department of Medical Physics and Biomedical Engineering}

\maketitle

% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
Accurate prediction of surgical duration and tool usage is essential for operating room scheduling and intraoperative decision support. We present a CNN-LSTM framework for surgical workflow analysis using the Cholec80 dataset. For duration prediction (Task A), we predict remaining phase and surgery time, achieving 7.27 and 13.80 minutes MAE respectively. For tool detection (Task B), we investigate whether \textit{predicted} timing from Task A improves performance. Comparing a baseline against a time-augmented model using frozen Task A predictions, we find that predicted temporal context (0.954 mAP) performs worse than the baseline (0.960 mAP), while ground-truth elapsed time shows improvement (0.962 mAP)---indicating that Task A predictions require higher accuracy before benefiting downstream tasks.

\keywords{Surgical workflow analysis \and Duration prediction \and Tool detection \and Deep learning \and LSTM}
\end{abstract}

% ============================================================================
% INTRODUCTION
% ============================================================================
\section{Introduction}

\subsection{Clinical Motivation}

Laparoscopic cholecystectomy is one of the most frequently performed surgical procedures worldwide, with over 1.2 million operations annually in the United States alone~\cite{twinanda2016endonet}. Despite its routine nature, surgical duration varies considerably between patients due to anatomical differences, complications, and surgeon experience. Accurate prediction of surgical duration and workflow progression has significant clinical value:

\begin{itemize}
    \item \textbf{Operating Room Scheduling}: Improved duration estimates enable better scheduling of subsequent procedures, reducing staff overtime and patient waiting times.
    \item \textbf{Resource Allocation}: Knowing which tools will be needed and when allows for better preparation of surgical instruments.
    \item \textbf{Intraoperative Decision Support}: Real-time awareness of surgical progress can alert teams to procedures running longer than expected, potentially indicating complications.
\end{itemize}

\subsection{Research Questions}

In this work, we address the following research questions:

\textbf{RQ1}: How does temporal context length affect surgical duration prediction accuracy? We hypothesize that longer sequences provide more information but may introduce noise from older, less relevant frames.

\textbf{RQ2}: Can multi-output prediction of all phase start and end times provide useful surgical timeline estimates beyond single-target duration regression?

\textbf{RQ3}: Does incorporating predicted temporal information (from Task A) improve downstream tool detection (Task B)? We hypothesize that knowing the surgical progress helps predict which tools are likely to be present, as certain tools are phase-specific (e.g., Clipper in ClippingCutting phase, SpecimenBag in GallbladderPackaging).

% ============================================================================
% METHODS
% ============================================================================
\section{Methods}

\subsection{Architecture}

We design a unified video-based pipeline for surgical workflow analysis consisting of three stages: (1) frame-level visual feature extraction, (2) temporal modelling across frame sequences, and (3) task-specific prediction heads. Video frames are sampled at 1 fps and grouped into fixed-length sequences of $T$ frames.

\textbf{Visual Features:} We use ResNet-50~\cite{he2016deep} pretrained on ImageNet, producing a \textbf{2048-dimensional feature vector per frame}. To balance adaptation and regularisation, we apply \textbf{partial fine-tuning}: early layers are frozen while the final residual block (Layer4) is trainable, allowing adaptation to laparoscopic imagery while preserving robust low-level features~\cite{yosinski2014transferable}.

\textbf{Temporal Modelling:} Frame features are processed by a \textbf{two-layer LSTM}~\cite{hochreiter1997lstm} with hidden dimension \textbf{256}. The final hidden state serves as the shared temporal representation for all downstream predictions, capturing both short-term patterns (e.g., tool motion) and longer-term trends (e.g., phase progression).

\subsection{Task A: Duration Prediction}

For Task A, the model receives a sequence of frames along with \textbf{normalised elapsed time} (divided by maximum surgery duration) concatenated to CNN features before the LSTM~\cite{czempiel2020tecno}. This temporal anchor helps the model understand where in the procedure it is, providing context that pure visual features lack.

The LSTM output feeds into \textbf{multiple prediction heads}:
\begin{itemize}
    \item \textbf{Duration heads}: Remaining time for current phase and total surgery
    \item \textbf{Timeline heads}: Start/end times for all 7 phases (relative to current time)
    \item \textbf{Progress head}: Sigmoid output predicting surgery completion (0$\to$1), serving as an auxiliary task that encourages temporally-aware representations
\end{itemize}

This multi-output design provides richer scheduling information than single duration values, inspired by multi-task learning approaches in surgical workflow analysis~\cite{twinanda2016endonet}. All regression outputs use \textbf{L1 loss}, which is robust to duration variability and yields errors directly interpretable in minutes.

\subsection{Task B: Tool Detection with Temporal Information}

Task B evaluates whether temporal information from Task A can improve a downstream task. We select \textbf{surgical tool detection}, as tool usage is strongly correlated with surgical phase~\cite{twinanda2016endonet}.

The same CNN--LSTM backbone is reused with two task-specific heads:
\begin{itemize}
    \item A \textbf{multi-label tool detection head} with 7 sigmoid outputs
    \item A \textbf{phase classification head} with 7 softmax outputs (auxiliary task)
\end{itemize}

This multi-task design follows EndoNet~\cite{twinanda2016endonet}, leveraging tool-phase correlations. Binary cross-entropy with \textbf{class weighting} addresses tool imbalance (Grasper: 60.6\% vs Scissors: 1.9\%).

\textbf{Experimental Conditions:} We compare three configurations to isolate the effect of temporal information:
\begin{itemize}
    \item \textbf{Baseline}: CNN-LSTM with no time features
    \item \textbf{Elapsed time}: Ground-truth elapsed time concatenated to CNN features before the LSTM, serving as an upper bound for what temporal context can provide
    \item \textbf{Predicted time}: Surgery remaining time predicted by our best Task A model (LSTM, $T=30$, trained with partial fine-tuning), testing realistic deployment where ground-truth is unavailable
\end{itemize}

For time-augmented models, the temporal feature (either elapsed or predicted time) is concatenated to the 2048-dimensional CNN features for each frame, creating a 2049-dimensional input to the LSTM.

\textbf{Task A Model Usage:} For the predicted time condition, we use our best-performing Task A model, which was trained with partial fine-tuning (unfrozen Layer4) and sequence length $T=30$. During Task B training, this Task A model is \textbf{frozen} (inference only, no gradient updates). We freeze it for two reasons: (1) to ensure consistent time predictions throughout Task B training, and (2) to evaluate whether Task A predictions \textit{as-is} provide useful signal, simulating a realistic pipeline where Task A is pre-trained and deployed separately.

\subsection{Training Strategy}

\textbf{Optimisation:} We use AdamW~\cite{loshchilov2019adamw} with ReduceLROnPlateau scheduling, reducing the learning rate when validation performance plateaus.

\textbf{Regularisation:} We combine dropout between LSTM layers~\cite{gal2016dropout}, weight decay, early stopping, and partial CNN freezing to prevent overfitting.

\textbf{Data Augmentation:} Following standard practices in surgical video analysis~\cite{twinanda2016endonet}, we apply random cropping, horizontal flipping, and colour jitter to simulate OR visual variations. Vertical flipping is not used as surgical orientation is clinically meaningful.

% ============================================================================
% EXPERIMENTS
% ============================================================================
\section{Experiments}

\subsection{Dataset}

We use the \textbf{Cholec80} dataset~\cite{twinanda2016endonet}, consisting of 80 videos of laparoscopic cholecystectomy performed by 13 surgeons. Video durations range from approximately 20 to 80 minutes. The dataset includes frame-level annotations for 7 surgical phases (Preparation, Calot Triangle Dissection, Clipping and Cutting, Gallbladder Dissection, Gallbladder Packaging, Cleaning and Coagulation, Gallbladder Retraction) and 7 surgical tools (Grasper, Bipolar, Hook, Scissors, Clipper, Irrigator, SpecimenBag).

\subsection{Experimental Setup}

We split the data by video (not by frame) to prevent data leakage from temporal correlations: 48 videos for training (60\%), 16 for validation (20\%), and 16 for testing (20\%). Frames are extracted at 1 fps from the original 25 fps videos and normalised using ImageNet statistics. Training uses learning rate $10^{-4}$, weight decay $10^{-4}$, batch size 8, and early stopping with patience of 5 epochs.

\subsection{Evaluation Metrics}

For \textbf{Task A} (duration prediction), we use Mean Absolute Error (MAE) measured in minutes. MAE provides directly interpretable errors in clinical time units---a prediction error of ``X minutes'' is immediately meaningful for operating room scheduling. Unlike RMSE, MAE is robust to outliers from unusually long or complicated surgeries.

For \textbf{Task B} (tool detection), we use Mean Average Precision (mAP), the standard metric for multi-label classification that summarises detection quality across all tools while handling class imbalance. We also report per-tool Average Precision (AP) to identify which specific tools are most challenging to detect---this is clinically relevant as rare tools like SpecimenBag and Scissors require particular attention. For the auxiliary phase classification task, we report classification accuracy.

\subsection{Task A: Duration Prediction Experiments}

We conduct a series of experiments to optimise duration prediction, systematically investigating architectural choices, temporal context, and regularisation.

\textbf{Frozen vs Unfrozen CNN:} We first compare two transfer learning strategies: (1) fully frozen CNN where all ResNet-50 weights are fixed, and (2) partial fine-tuning where the final residual block (Layer4) is trainable. This investigates whether the domain shift from ImageNet to laparoscopic imagery requires CNN adaptation, or whether frozen features suffice.

\textbf{Sequence Length Ablation:} We investigate how much temporal context is needed for accurate duration prediction by comparing sequence lengths $T \in \{15, 30, 60\}$ frames (corresponding to 15, 30, and 60 seconds at 1 fps). Shorter sequences may lack sufficient context about surgical progression, while longer sequences risk introducing noise from outdated visual information.

\textbf{Regularisation:} During initial experiments, we observed overfitting where training loss continued decreasing while validation loss plateaued or increased. To address this, we systematically applied multiple regularisation techniques: dropout between LSTM layers ($p=0.3$) and before prediction heads ($p=0.5$), weight decay ($10^{-4}$), early stopping based on validation performance, and partial CNN freezing to constrain the hypothesis space.

\textbf{Temporal Architecture Comparison:} To evaluate whether alternative temporal models could improve performance, we compare the LSTM-based architecture against a Transformer encoder with equivalent capacity (same hidden dimension, comparable parameter count). This tests whether the self-attention mechanism offers advantages over recurrent processing for surgical video sequences.

\subsection{Task B: Tool Detection Experiments}

Task B investigates whether predicted temporal information from Task A can improve surgical tool detection---the core question of whether duration estimates are useful for downstream tasks.

\textbf{Baseline Multi-task Model:} We first establish a baseline using the CNN-LSTM backbone with multi-task learning (tool detection + phase classification) but no temporal features. This follows the EndoNet approach~\cite{twinanda2016endonet}, exploiting the correlation between tools and surgical phases.

\textbf{Time-Augmented Model:} We then augment the model with predicted surgery remaining time from a frozen Task A model. Critically, we use \textit{predicted} time rather than ground-truth elapsed time---this reflects a realistic deployment scenario where true timing is unavailable, and directly tests whether Task A predictions provide useful context. The frozen Task A model generates predictions for each training batch, which are concatenated with CNN features before the LSTM.

\textbf{Per-Tool Analysis:} We analyse detection performance for each tool individually to understand which tools benefit from temporal information. We hypothesise that phase-specific tools (Clipper during ClippingCutting, SpecimenBag during GallbladderPackaging) will show greater improvement, as knowing surgical progress helps predict their presence.

% ============================================================================
% RESULTS
% ============================================================================
\section{Results}

\subsection{Task A: Duration Prediction}

\subsubsection{CNN Fine-tuning}
Comparing frozen CNN against partial fine-tuning (unfreezing Layer4), we found similar final performance but more stable training with fine-tuning. The frozen CNN showed higher variance across epochs, while unfrozen Layer4 converged smoothly. We use the fine-tuned variant for subsequent experiments.

\subsubsection{Sequence Length Ablation}
We tested temporal context lengths $T \in \{15, 30, 60\}$ frames (Table~\ref{tab:seq_length}).

\begin{table}[h]
\centering
\caption{Effect of sequence length on duration prediction (MAE in minutes).}
\label{tab:seq_length}
\begin{tabular}{lcc}
\toprule
Sequence Length & Phase Remaining & Surgery Remaining \\
\midrule
$T = 15$ & 7.48 & 14.03 \\
$T = 30$ & \textbf{7.27} & \textbf{13.80} \\
$T = 60$ & 7.52 & 14.48 \\
\bottomrule
\end{tabular}
\end{table}

$T=15$ provides insufficient context---the model cannot observe enough surgical progression to learn meaningful patterns. $T=60$ causes overfitting; outdated visual information introduces noise rather than useful signal, and training terminated early. $T=30$ achieves the optimal balance, capturing recent surgical activity without temporal dilution.

\subsubsection{Final Task A Performance}
Table~\ref{tab:task_a} shows complete duration prediction results at $T=30$, including the LSTM vs Transformer comparison.

\begin{table}[h]
\centering
\caption{Task A results at $T=30$: LSTM vs Transformer (MAE in minutes).}
\label{tab:task_a}
\begin{tabular}{lcccc}
\toprule
Model & Phase Rem. & Surgery Rem. & Phase Starts & Phase Ends \\
\midrule
LSTM & \textbf{7.27} & \textbf{13.80} & \textbf{7.28} & \textbf{9.12} \\
Transformer & 7.59 & 13.84 & 7.32 & 9.31 \\
\bottomrule
\end{tabular}
\end{table}

The LSTM outperforms the Transformer across all metrics. The Transformer exhibited severe overfitting (train MAE: 1.79 min vs val MAE: 7.85 min), triggering early stopping after 4 epochs. For limited data and short sequences, LSTM's sequential inductive bias proves advantageous over learned attention patterns.

\textbf{Clinical context:} Surgery remaining MAE of $\sim$14 minutes represents $\sim$30\% error for a typical 45-minute cholecystectomy---useful for approximate scheduling but highlighting the inherent difficulty of duration prediction from visual information alone.

\subsection{Task B: Tool Detection}

\subsubsection{Time Augmentation Comparison}
Table~\ref{tab:taskb} compares three conditions: baseline (no time), ground-truth elapsed time, and predicted time from Task A.

\begin{table}[h]
\centering
\caption{Task B tool detection results.}
\label{tab:taskb}
\begin{tabular}{lcc}
\toprule
Model & mAP & Phase Accuracy \\
\midrule
Baseline (no time) & \textbf{0.960} & 89.2\% \\
+ Elapsed time (ground truth) & 0.962 & \textbf{90.5\%} \\
+ Predicted time (from Task A) & 0.954 & 88.6\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key finding:} Predicted time from Task A \textit{hurts} performance (0.954 vs 0.960 mAP), while ground-truth elapsed time \textit{helps} (0.962 mAP). This indicates that accurate temporal context benefits tool detection, but Task A's $\sim$14 minute MAE introduces more noise than signal.

\subsubsection{Per-Tool Analysis}

\begin{table}[h]
\centering
\caption{Per-tool Average Precision (baseline model).}
\label{tab:per_tool}
\begin{tabular}{lcc}
\toprule
Tool & Frequency & AP \\
\midrule
Hook & 56.0\% & 0.994 \\
SpecimenBag & 6.8\% & 0.976 \\
Irrigator & 6.7\% & 0.968 \\
Bipolar & 5.3\% & 0.962 \\
Clipper & 3.6\% & 0.958 \\
Grasper & 60.5\% & 0.918 \\
Scissors & 1.8\% & 0.906 \\
\bottomrule
\end{tabular}
\end{table}

Hook achieves highest AP (0.994) due to its distinctive appearance. Grasper (60.5\% frequency) has lower AP (0.918) because it appears throughout surgery, making \textit{absence} hard to predict. Scissors shows lowest AP (0.906), reflecting the challenge of rare tool detection (1.8\%) despite class weighting.

% ============================================================================
% DISCUSSION
% ============================================================================
\section{Discussion}

\textbf{RQ1 (Temporal Context):} The optimal sequence length balances information and noise. Too short (15 frames) provides insufficient surgical context; too long (60 frames) introduces outdated information that harms generalisation. The 30-second window aligns with the temporal dynamics of surgical phases, which evolve gradually over tens of seconds.

\textbf{RQ2 (Phase Timeline Prediction):} Multi-output prediction successfully estimates complete surgical timelines. Phase start/end predictions (7--9 min MAE) are more accurate than surgery-level predictions (14 min MAE), suggesting that local phase dynamics are easier to model than global surgery progression. This granularity is valuable for OR scheduling---knowing when specific phases will occur enables targeted resource preparation.

\textbf{RQ3 (Task Chaining):} Our central hypothesis---that predicted temporal information improves downstream tasks---is \textit{not supported}. This negative result reveals a critical insight: \textbf{task chaining requires upstream predictions to exceed a quality threshold}. Ground-truth timing helps tool detection, proving the concept works. But predicted timing with $\sim$30\% error introduces more confusion than signal---the model learns to distrust or is misled by unreliable temporal cues.

\textbf{LSTM vs Transformer:} The Transformer's failure despite equivalent capacity highlights that architectural sophistication does not guarantee better performance. For limited data and short sequences, LSTM's built-in sequential bias outperforms learned attention. This suggests practitioners should prefer simpler architectures when data is scarce.

\textbf{Limitations:} Single-dataset evaluation (Cholec80) limits generalisability. Class imbalance affects rare tools despite weighting. 1 fps sampling may miss brief tool appearances. The 30\% duration prediction error may be an inherent limitation of visual-only approaches.

% ============================================================================
% CONCLUSION
% ============================================================================
\section{Conclusion}

We presented a CNN-LSTM framework for surgical workflow analysis on Cholec80. Task A (duration prediction) achieves clinically useful accuracy for phase-level predictions, with multi-output timeline estimation providing richer scheduling information than single-target regression. Task B (tool detection) achieves strong baseline performance, but augmentation with predicted temporal context \textit{hurts} rather than helps---a meaningful negative result.

Our key insight is that \textbf{task chaining in surgical AI requires upstream predictions to meet quality thresholds}. Accurate timing information benefits downstream tasks, but noisy predictions introduce harmful confusion. This has practical implications: practitioners should validate upstream model accuracy before building dependent pipelines.

Future work should explore: (1) improving duration prediction through multi-modal inputs (e.g., instrument kinematics), (2) uncertainty-aware time predictions that signal reliability to downstream models, (3) end-to-end joint training rather than frozen task chaining, and (4) \textbf{surgical anomaly detection}---leveraging the learned tool-phase correlations to flag when a tool appears in an unexpected phase, potentially indicating procedural errors or complications requiring intervention.

% ============================================================================
% REFERENCES
% ============================================================================
\bibliographystyle{splncs04}
\begin{thebibliography}{9}

\bibitem{twinanda2016endonet}
Twinanda, A.P., Shehata, S., Mutter, D., Marescaux, J., De Mathelin, M., Padoy, N.: EndoNet: A deep architecture for recognition tasks on laparoscopic videos. IEEE Transactions on Medical Imaging \textbf{36}(1), 86--97 (2016)

\bibitem{czempiel2020tecno}
Czempiel, T., Paber, M., Shehata, S., Stypulkowski, M., Ostler, D., Navab, N., Tombari, F.: TeCNO: Surgical phase recognition with multi-stage temporal convolutional networks. In: MICCAI 2020. pp. 343--352 (2020)

\bibitem{he2016deep}
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR. pp. 770--778 (2016)

\bibitem{yosinski2014transferable}
Yosinski, J., Clune, J., Bengio, Y., Lipson, H.: How transferable are features in deep neural networks? In: NeurIPS. pp. 3320--3328 (2014)

\bibitem{loshchilov2019adamw}
Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: ICLR (2019)

\bibitem{gal2016dropout}
Gal, Y., Ghahramani, Z.: A theoretically grounded application of dropout in recurrent neural networks. In: NeurIPS. pp. 1019--1027 (2016)

\bibitem{hochreiter1997lstm}
Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Computation \textbf{9}(8), 1735--1780 (1997)

\end{thebibliography}

\end{document}
