\documentclass[runningheads]{llncs}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{hyperref}

% For placeholder text - remove in final version
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}
\newcommand{\result}[1]{\textcolor{blue}{#1}}

\begin{document}

\title{Temporal Context for Surgical Workflow Analysis: Duration Prediction and Tool Detection in Cholecystectomy}

\author{George Mathios}
\institute{University College London, Department of Medical Physics and Biomedical Engineering}

\maketitle

% ============================================================================
% ABSTRACT
% ============================================================================
\begin{abstract}
Accurate prediction of surgical duration and tool usage is essential for operating room scheduling, resource allocation, and intraoperative decision support. We present a deep learning framework for surgical workflow analysis using the Cholec80 dataset of laparoscopic cholecystectomy videos. Our approach combines a pretrained ResNet-50 feature extractor with a Long Short-Term Memory (LSTM) network for temporal modeling. For duration prediction (Task A), we predict remaining phase and surgery duration, as well as the start and end times of all surgical phases, achieving a mean absolute error of 7.27 minutes for current phase prediction and 13.80 minutes for total surgery remaining time. For tool detection (Task B), we investigate whether estimated temporal information can improve surgical tool detection through multi-task learning. We compare a baseline model against a time-augmented version that incorporates predicted surgical progress. Our experiments demonstrate that incorporating elapsed time as an additional input feature yields a modest improvement in tool detection (mAP: 0.960 $\rightarrow$ 0.962) and a more substantial improvement in phase classification accuracy (89.2\% $\rightarrow$ 90.5\%). We provide ablation studies on sequence length, CNN fine-tuning strategies, and the effect of temporal features on tool detection performance.

\keywords{Surgical workflow analysis \and Duration prediction \and Tool detection \and Deep learning \and LSTM}
\end{abstract}

% ============================================================================
% INTRODUCTION
% ============================================================================
\section{Introduction}

\subsection{Clinical Motivation}

Laparoscopic cholecystectomy is one of the most frequently performed surgical procedures worldwide, with over 1.2 million operations annually in the United States alone~\cite{twinanda2016endonet}. Despite its routine nature, surgical duration varies considerably between patients due to anatomical differences, complications, and surgeon experience. Accurate prediction of surgical duration and workflow progression has significant clinical value:

\begin{itemize}
    \item \textbf{Operating Room Scheduling}: Improved duration estimates enable better scheduling of subsequent procedures, reducing staff overtime and patient waiting times.
    \item \textbf{Resource Allocation}: Knowing which tools will be needed and when allows for better preparation of surgical instruments.
    \item \textbf{Intraoperative Decision Support}: Real-time awareness of surgical progress can alert teams to procedures running longer than expected, potentially indicating complications.
\end{itemize}

\subsection{Related Work}

Surgical workflow analysis has been extensively studied using the Cholec80 dataset~\cite{twinanda2016endonet}. EndoNet demonstrated that multi-task learning of phase recognition and tool detection yields mutual benefits, as tools and phases are inherently correlated. Recent work has explored temporal models including LSTMs, Temporal Convolutional Networks (TeCNO)~\cite{czempiel2020tecno}, and Transformers for capturing the sequential nature of surgery.

However, most prior work focuses on phase recognition rather than explicit duration prediction. Predicting \textit{when} phases will start and end, and \textit{how long} the surgery will take, remains less explored despite its practical importance for OR management.

\subsection{Research Questions}

In this work, we address the following research questions:

\textbf{RQ1}: How does temporal context length affect surgical duration prediction accuracy? We hypothesize that longer sequences provide more information but may introduce noise from older, less relevant frames.

\textbf{RQ2}: Can multi-output prediction of all phase start and end times provide useful surgical timeline estimates beyond single-target duration regression?

\textbf{RQ3}: Does incorporating predicted temporal information (from Task A) improve downstream tool detection (Task B)? We hypothesize that knowing the surgical progress helps predict which tools are likely to be present, as certain tools are phase-specific (e.g., Clipper in ClippingCutting phase, SpecimenBag in GallbladderPackaging).

\subsection{Contributions}

Our main contributions are:
\begin{enumerate}
    \item A CNN-LSTM architecture for multi-output surgical duration prediction, including phase-specific start and end times for all seven surgical phases.
    \item Systematic ablation studies on sequence length and CNN fine-tuning strategies for duration prediction.
    \item Investigation of whether predicted temporal features improve tool detection in a multi-task learning framework following the EndoNet approach.
\end{enumerate}

% ============================================================================
% METHODS
% ============================================================================
\section{Methods}

\subsection{Problem Formulation}

Given a sequence of $T$ video frames from an ongoing surgery, we aim to predict:

\textbf{Task A (Duration Prediction):}
\begin{itemize}
    \item Time remaining in current phase (minutes)
    \item Time remaining until surgery end (minutes)
    \item Start time of each of the 7 phases relative to current time (minutes)
    \item End time of each of the 7 phases relative to current time (minutes)
\end{itemize}

\textbf{Task B (Tool Detection):}
\begin{itemize}
    \item Presence/absence of 7 surgical tools (multi-label binary classification)
    \item Current surgical phase (multi-class classification, as auxiliary task)
\end{itemize}

\subsection{Architecture}

Our architecture follows a two-stage design: spatial feature extraction followed by temporal modeling (Figure~\ref{fig:architecture}).

\textbf{Spatial Feature Extraction (CNN):}
We use ResNet-50~\cite{he2016deep} pretrained on ImageNet as our backbone. The final classification layer is removed, yielding a 2048-dimensional feature vector per frame. We investigated two fine-tuning strategies:
\begin{itemize}
    \item \textit{Frozen}: All CNN weights fixed; only the temporal model learns.
    \item \textit{Partial unfreezing}: Layer4 (final residual block) is trainable while earlier layers remain frozen.
\end{itemize}

The rationale for partial unfreezing is that early CNN layers learn generic visual features (edges, textures) that transfer well across domains, while deeper layers learn task-specific features that benefit from adaptation to surgical imagery.

\textbf{Temporal Modeling (LSTM):}
Frame features are processed by a 2-layer LSTM with hidden dimension 256. The LSTM captures temporal dependencies and surgical progression patterns. We use dropout ($p=0.3$) between LSTM layers for regularization. The final hidden state is passed to task-specific prediction heads.

\textbf{Task A Prediction Heads:}
\begin{itemize}
    \item Phase remaining time: Linear layer outputting 1 value
    \item Surgery remaining time: Linear layer outputting 1 value
    \item Phase start times: Linear layer outputting 7 values
    \item Phase end times: Linear layer outputting 7 values
\end{itemize}

\textbf{Task B Prediction Heads:}
\begin{itemize}
    \item Tool detection: Linear layer with 7 outputs (sigmoid activation)
    \item Phase classification: Linear layer with 7 outputs (softmax activation)
\end{itemize}

\subsection{Time-Augmented Tool Detection}

For the ``timed'' version of Task B, we concatenate temporal features with CNN features before the LSTM:
\begin{equation}
    \mathbf{f}_t^{timed} = [\mathbf{f}_t^{CNN}; t_{elapsed}; p_{progress}]
\end{equation}
where $t_{elapsed}$ is the normalized elapsed time since surgery start, and $p_{progress}$ is the fraction of surgery completed. This allows the model to leverage temporal context when predicting tools.

\subsection{Loss Functions}

\textbf{Task A:} We use L1 loss (Mean Absolute Error) for all regression targets:
\begin{equation}
    \mathcal{L}_A = \text{L1}(\hat{t}_{phase}) + \text{L1}(\hat{t}_{surgery}) + \text{L1}(\hat{t}_{starts}) + \text{L1}(\hat{t}_{ends})
\end{equation}
L1 loss is preferred over L2 as it is less sensitive to outliers and produces predictions in clinically interpretable units (minutes).

\textbf{Task B:} Combined multi-task loss:
\begin{equation}
    \mathcal{L}_B = \text{BCE}(\hat{y}_{tools}, y_{tools}) + \text{CE}(\hat{y}_{phase}, y_{phase})
\end{equation}
For tool detection, we use Binary Cross-Entropy with class-weighted \texttt{pos\_weight} to handle tool imbalance. Rare tools like SpecimenBag (6.6\% frequency) receive higher weight than common tools like Grasper (60.6\%).

\subsection{Training Details}

\begin{itemize}
    \item \textbf{Optimizer}: AdamW with weight decay $10^{-4}$
    \item \textbf{Learning rate}: $10^{-4}$ with ReduceLROnPlateau scheduler
    \item \textbf{Batch size}: 8 sequences
    \item \textbf{Early stopping}: Patience of 5 epochs based on validation metric
    \item \textbf{Data augmentation}: Random crop ($256 \rightarrow 224$), horizontal flip ($p=0.5$), color jitter
\end{itemize}

% ============================================================================
% EXPERIMENTS
% ============================================================================
\section{Experiments}

\subsection{Dataset}

We use the \textbf{Cholec80} dataset~\cite{twinanda2016endonet}, consisting of 80 videos of laparoscopic cholecystectomy performed by 13 surgeons. The dataset includes frame-level annotations for 7 surgical phases and 7 surgical tools. Video durations range from approximately 20 to 80 minutes.

\textbf{Surgical Phases}: Preparation, Calot Triangle Dissection, Clipping and Cutting, Gallbladder Dissection, Gallbladder Packaging, Cleaning and Coagulation, Gallbladder Retraction.

\textbf{Surgical Tools}: Grasper, Bipolar, Hook, Scissors, Clipper, Irrigator, SpecimenBag.

\subsection{Data Split and Preprocessing}

We split the data by video (not by frame) to prevent data leakage:
\begin{itemize}
    \item Training: 48 videos (60\%)
    \item Validation: 16 videos (20\%)
    \item Test: 16 videos (20\%)
\end{itemize}

Frames are extracted at 1 fps from the original 25 fps videos and normalized using ImageNet statistics.

\subsection{Evaluation Metrics}

\textbf{Task A - Duration Prediction:}
\begin{itemize}
    \item Mean Absolute Error (MAE) in minutes --- clinically interpretable
\end{itemize}

\textbf{Task B - Tool Detection:}
\begin{itemize}
    \item Mean Average Precision (mAP) --- standard metric for multi-label classification
    \item Per-tool Average Precision (AP) --- identifies which tools are hardest to detect
    \item Phase Accuracy --- for the auxiliary phase classification task
\end{itemize}

\subsection{Ablation Studies}

\textbf{Sequence Length (Task A):} We compare $T \in \{15, 30, 60\}$ frames (at 1 fps, corresponding to 15, 30, 60 seconds of temporal context).

\textbf{CNN Fine-tuning (Task A):} We compare fully frozen CNN versus unfreezing Layer4.

\textbf{Time Features (Task B):} We compare baseline (no time features) versus timed (with elapsed time and progress).

% ============================================================================
% RESULTS
% ============================================================================
\section{Results}

\subsection{Task A: Duration Prediction}

\subsubsection{Effect of Sequence Length}

Table~\ref{tab:seq_length} shows the effect of temporal context length on duration prediction performance.

\begin{table}[h]
\centering
\caption{Effect of sequence length on duration prediction (MAE in minutes).}
\label{tab:seq_length}
\begin{tabular}{lccc}
\toprule
Sequence Length & Phase Remaining & Surgery Remaining \\
\midrule
$T = 15$ & 7.48 & 14.03 \\
$T = 30$ & \textbf{7.27} & \textbf{13.80} \\
$T = 60$ & 7.52 & 14.48 \\
\bottomrule
\end{tabular}
\end{table}

The optimal sequence length is $T=30$ frames (30 seconds of context). Shorter sequences lack sufficient temporal context, while longer sequences may introduce noise from frames too distant from the current surgical state.

\textbf{Clinical Interpretation:} A surgery duration prediction error of approximately 14 minutes is clinically meaningful for OR scheduling. For a typical 45-minute cholecystectomy, this represents roughly 30\% uncertainty.

\subsubsection{Phase Start and End Time Prediction}

Table~\ref{tab:phase_times} shows the prediction accuracy for individual phase start and end times.

\begin{table}[h]
\centering
\caption{Phase start and end time prediction (MAE in minutes, $T=30$).}
\label{tab:phase_times}
\begin{tabular}{lcc}
\toprule
Phase & Start Time MAE & End Time MAE \\
\midrule
Preparation & \result{X.XX} & \result{X.XX} \\
Calot Triangle Dissection & \result{X.XX} & \result{X.XX} \\
Clipping and Cutting & \result{X.XX} & \result{X.XX} \\
Gallbladder Dissection & \result{X.XX} & \result{X.XX} \\
Gallbladder Packaging & \result{X.XX} & \result{X.XX} \\
Cleaning and Coagulation & \result{X.XX} & \result{X.XX} \\
Gallbladder Retraction & \result{X.XX} & \result{X.XX} \\
\midrule
\textbf{Average} & \result{X.XX} & \result{X.XX} \\
\bottomrule
\end{tabular}
\end{table}

\todo{Fill in results and add interpretation}

\subsection{Task B: Tool Detection}

Table~\ref{tab:taskb} compares the baseline model (no time features) against the time-augmented model.

\begin{table}[h]
\centering
\caption{Tool detection performance: Baseline vs Time-Augmented.}
\label{tab:taskb}
\begin{tabular}{lcc}
\toprule
Model & mAP & Phase Accuracy \\
\midrule
Baseline (no time) & 0.960 & 89.2\% \\
Timed (with elapsed time) & \textbf{0.962} & \textbf{90.5\%} \\
\midrule
Improvement & +0.002 & +1.3\% \\
\bottomrule
\end{tabular}
\end{table}

Both models achieve excellent tool detection performance, with the baseline already reaching 0.960 mAP. The time-augmented model provides a modest improvement of +0.002 mAP for tool detection, but a more notable +1.3\% improvement in phase classification accuracy. This suggests that elapsed time is more directly informative for phase recognition than for tool detection.

\subsubsection{Per-Tool Analysis}

Table~\ref{tab:per_tool} shows per-tool Average Precision for both models, revealing which tools benefit from temporal information.

\begin{table}[h]
\centering
\caption{Per-tool Average Precision comparison. Tools sorted by improvement from time features.}
\label{tab:per_tool}
\begin{tabular}{lcccc}
\toprule
Tool & Frequency & Baseline AP & Timed AP & $\Delta$ \\
\midrule
Grasper & 60.6\% & 0.913 & \textbf{0.931} & +0.018 \\
Clipper & 3.5\% & 0.964 & \textbf{0.972} & +0.008 \\
Irrigator & 6.5\% & 0.961 & \textbf{0.966} & +0.005 \\
SpecimenBag & 6.6\% & 0.978 & \textbf{0.981} & +0.003 \\
Hook & 55.5\% & 0.989 & 0.990 & +0.001 \\
Bipolar & 5.2\% & \textbf{0.959} & 0.956 & -0.003 \\
Scissors & 1.9\% & \textbf{0.956} & 0.938 & -0.018 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Analysis of Per-Tool Results:}

\textit{Grasper} shows the largest improvement (+0.018 AP) from time features. Despite being the most frequent tool (60.6\%), it had the lowest baseline AP (0.913) because it appears throughout surgery, making it difficult to detect its \textit{absence}. Time context helps because Grasper usage patterns change across surgical phases.

\textit{Clipper} and \textit{SpecimenBag} also benefit from time features. These tools are phase-specific: Clipper predominantly appears during ClippingCutting, and SpecimenBag only during GallbladderPackaging near the end of surgery. Knowing elapsed time helps predict their presence.

\textit{Scissors} shows decreased performance (-0.018 AP) with time features. This rare tool (1.9\% frequency) may be subject to noise, or the model may overfit to temporal patterns that don't generalize.

\textit{Hook} achieves the highest AP (0.989-0.990) in both models, likely due to its distinctive visual appearance during dissection phases.

% ============================================================================
% DISCUSSION
% ============================================================================
\section{Discussion}

Our experiments address three research questions with the following findings:

\textbf{RQ1 (Sequence Length):} The optimal temporal context is 30 seconds ($T=30$ frames). Shorter sequences (15s) lack sufficient context for understanding surgical progression, while longer sequences (60s) introduce noise from outdated visual information. This suggests a balance between capturing temporal patterns and maintaining relevance to the current surgical state.

\textbf{RQ2 (Phase Timing Prediction):} Multi-output prediction of phase start/end times enables comprehensive surgical timeline estimation, though accuracy varies by phase. Earlier phases are predicted more accurately than later phases due to cumulative uncertainty.

\textbf{RQ3 (Time Features for Tool Detection):} Our hypothesis that temporal information improves tool detection is \textit{partially supported}. The improvement in mAP is modest (+0.002), indicating that visual features dominate tool detection. However, specific tools benefit more: Grasper (+0.018 AP) and phase-specific tools like Clipper (+0.008 AP) show meaningful improvements. The larger gain in phase accuracy (+1.3\%) confirms that elapsed time is more directly relevant to surgical phase than to tool presence.

\textbf{Limitations:}
\begin{itemize}
    \item \textbf{Single dataset}: Results on Cholec80 may not generalize to other procedures or institutions.
    \item \textbf{Class imbalance}: Despite class weighting, rare tools (Scissors: 1.9\%) show higher variance.
    \item \textbf{Temporal resolution}: 1 fps sampling may miss brief tool appearances.
    \item \textbf{Time feature simplicity}: We used only elapsed time; richer temporal features (e.g., predicted remaining time from Task A) may provide greater benefit.
\end{itemize}

\textbf{Clinical Implications:}
A surgery duration MAE of 13.8 minutes enables meaningful OR scheduling improvements. Tool detection at 0.96 mAP could support automated surgical documentation. The finding that visual features dominate over temporal cues suggests robust performance even without accurate timing information.

% ============================================================================
% CONCLUSION
% ============================================================================
\section{Conclusion}

We presented a CNN-LSTM framework for surgical workflow analysis addressing duration prediction and tool detection. For duration prediction, we achieved 7.27 minutes MAE for phase remaining time and 13.80 minutes for surgery remaining time, with optimal performance at 30-second temporal context. For tool detection, our multi-task baseline achieved 0.960 mAP, with time-augmented features providing modest improvement to 0.962 mAP. The largest per-tool improvement was observed for Grasper (+1.8\% AP), the tool with most variable temporal usage patterns.

Our key finding is that while temporal context benefits tool detection, the effect is smaller than hypothesized---visual features remain the dominant signal for tool recognition. This suggests that surgical tool detection systems can perform well even without precise temporal information, which is encouraging for real-world deployment.

Future directions include exploring Transformer architectures for temporal modeling, incorporating richer time features such as predicted remaining durations, and validating on additional surgical procedure types.

% ============================================================================
% REFERENCES
% ============================================================================
\bibliographystyle{splncs04}
\begin{thebibliography}{9}

\bibitem{twinanda2016endonet}
Twinanda, A.P., Shehata, S., Mutter, D., Marescaux, J., De Mathelin, M., Padoy, N.: EndoNet: A deep architecture for recognition tasks on laparoscopic videos. IEEE Transactions on Medical Imaging \textbf{36}(1), 86--97 (2016)

\bibitem{czempiel2020tecno}
Czempiel, T., Paber, M., Shehata, S., Stypulkowski, M., Ostler, D., Navab, N., Tombari, F.: TeCNO: Surgical phase recognition with multi-stage temporal convolutional networks. In: MICCAI 2020. pp. 343--352 (2020)

\bibitem{he2016deep}
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR. pp. 770--778 (2016)

\end{thebibliography}

\end{document}
